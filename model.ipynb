{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境 tf2.0\n",
    "\n",
    "# 此可忽略\n",
    "# !git clone https://[帳號]:[密碼]@github.com/alex00252141/T-brain.git\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/training_data/chinese_L-12_H-768_A-12 .\n",
    "\n",
    "# 重要!! 請安裝以下資料\n",
    "# !git clone https://github.com/huggingface/transformers\n",
    "# !cd transformers\n",
    "# !pip install ./transformers/.\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/training_data/Model_BIO . #此落無法透過指令安裝 可從gs上面下載 gs://tbrain-tsmc/training_data/Model_BIO\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/Model_BIO_2 . \n",
    "# !gsutil -m cp -R ./day2.xlsx gs://tbrain-tsmc/training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('Model_BIO_2', num_labels=3)\n",
    "model = TFBertForTokenClassification.from_pretrained('Model_BIO_2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "\n",
    "def extract_name(text, label):\n",
    "    answers = set()\n",
    "    answer = ''\n",
    "    \n",
    "    for i in np.where(label>0)[0].tolist():\n",
    "        name = tokenizer.decode([text[i]])\n",
    "\n",
    "        if i in np.where(label==2)[0].tolist():\n",
    "            if answer != \"\":\n",
    "                answers.add(answer)\n",
    "            \n",
    "            answer = name\n",
    "        else:\n",
    "            answer += name\n",
    "\n",
    "    return answers\n",
    "\n",
    "def extract_name_plus(text, label):\n",
    "    answers = set()\n",
    "    answer = ''\n",
    "    \n",
    "    for i in range(len(label)):\n",
    "        indx_lastname = 0\n",
    "        \n",
    "        if label[i] == 2:\n",
    "            answer ='' \n",
    "            answer = tokenizer.decode([text[i]])\n",
    "            i += 1\n",
    "            \n",
    "            while label[i]!=0:\n",
    "                answer += tokenizer.decode([text[i]])\n",
    "                \n",
    "                if label[i]==2:\n",
    "                    break\n",
    "                \n",
    "                i += 1          \n",
    "    \n",
    "        if len(answer)> 2 :\n",
    "            answers.add(answer)\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    return answers\n",
    "\n",
    "###  這邊改成輸入的字串 test_string = ?\n",
    "# %run DataProcessor.ipynb\n",
    "# dp = DataProcessor()\n",
    "# tid =  165 #72 139 150(空)\n",
    "# titles, names, contexts = dp.get_all()\n",
    "tid =  118\n",
    "test_string  = df['News'][tid]\n",
    "start = time.time()\n",
    "####\n",
    "\n",
    "MAX_LEN = 512\n",
    "part = len(test_string) // MAX_LEN\n",
    "i = 0\n",
    "answers = set()\n",
    "all_text = np.array([])\n",
    "all_label = np.array([])\n",
    "\n",
    "while i <= part:\n",
    "    if i == part:\n",
    "        inputs = tokenizer(test_string[i*MAX_LEN: ], return_tensors=\"tf\")\n",
    "    else:\n",
    "        inputs = tokenizer(test_string[i*MAX_LEN: (i+1)*MAX_LEN], return_tensors=\"tf\")\n",
    "        \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss, scores = outputs[:2]\n",
    "\n",
    "    all_text = np.append(all_text, input_ids[0].numpy())\n",
    "    all_label = np.append(all_label, np.argmax(scores, -1)[0])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "answers.update(extract_name_plus(all_text, all_label))\n",
    "\n",
    "### 最終結果 ###\n",
    "print(list(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('day2.xlsx')\n",
    "for i, s in enumerate(df['News']):\n",
    "    if  '陳政其' in s:\n",
    "        print(s)\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
