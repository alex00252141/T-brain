{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://tbrain-tsmc/training_data/Model_BIO/Model_BIO/config.json...\n",
      "Copying gs://tbrain-tsmc/training_data/Model_BIO/Model_BIO/tf_model.h5...\n",
      "Copying gs://tbrain-tsmc/training_data/Model_BIO/tf_model.h5...\n",
      "Copying gs://tbrain-tsmc/training_data/Model_BIO/config.json...\n",
      "| [4/4 files][780.8 MiB/780.8 MiB] 100% Done  12.3 MiB/s ETA 00:00:00           \n",
      "Operation completed over 4 objects/780.8 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# 環境 tf2.0\n",
    "\n",
    "# 此可忽略\n",
    "# !git clone https://[帳號]:[密碼]@github.com/alex00252141/T-brain.git\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/training_data/chinese_L-12_H-768_A-12 .\n",
    "\n",
    "# 重要!! 請安裝以下資料\n",
    "# !git clone https://github.com/huggingface/transformers\n",
    "# !cd transformers\n",
    "# !pip install ./transformers/.\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/training_data/Model_BIO . #此落無法透過指令安裝 可從gs上面下載 gs://tbrain-tsmc/training_data/Model_BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Model_BIO were not used when initializing TFBertForTokenClassification: ['dropout_227']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at Model_BIO and are newly initialized: ['dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('Model_BIO', num_labels=3)\n",
    "model = TFBertForTokenClassification.from_pretrained('Model_BIO', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8454856872558594\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "\n",
    "def extract_name(text, label):\n",
    "    l_label = np.argmax(label, -1)[0]\n",
    "    \n",
    "    answers = set()\n",
    "    answer = ''\n",
    "    \n",
    "    for i in np.where(l_label>0)[0].tolist():\n",
    "        name = tokenizer.decode([text.numpy()[i]])\n",
    "\n",
    "        if i in np.where(l_label==2)[0].tolist():\n",
    "            if answer != \"\":\n",
    "                answers.add(answer)\n",
    "                \n",
    "            answer = name\n",
    "        else:\n",
    "            answer += name\n",
    "\n",
    "    return answers\n",
    "\n",
    "###  這邊改成輸入的字串 test_string = ?\n",
    "%run DataProcessor.ipynb\n",
    "dp = DataProcessor()\n",
    "tid =  165 #72 139 150(空)\n",
    "titles, names, contexts = dp.get_all()\n",
    "test_string = titles[tid] + contexts[tid]\n",
    "###\n",
    "\n",
    "start = time.time()\n",
    "MAX_LEN = 256\n",
    "part = len(test_string) // MAX_LEN\n",
    "i = 0\n",
    "answers = set()\n",
    "\n",
    "while i <= part:\n",
    "    if i == part:\n",
    "        inputs = tokenizer(test_string[i*MAX_LEN: ], return_tensors=\"tf\")\n",
    "    else:\n",
    "        inputs = tokenizer(test_string[i*MAX_LEN: (i+1)*MAX_LEN], return_tensors=\"tf\")\n",
    "        \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss, scores = outputs[:2]\n",
    "    \n",
    "#     print(tokenizer.decode(input_ids.numpy()[np.argmax(scores, -1)>0]))\n",
    "    answers.update(extract_name(input_ids[0], scores))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "### 最終結果\n",
    "print(list(answers))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
