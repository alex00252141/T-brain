{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境 tf2.0\n",
    "\n",
    "# 此可忽略\n",
    "# !git clone https://alex00252141:good12258@github.com/alex00252141/T-brain.git\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/training_data/chinese_L-12_H-768_A-12 .\n",
    "\n",
    "# 重要!! 請安裝以下資料\n",
    "# !git clone https://github.com/huggingface/transformers\n",
    "# !cd transformers\n",
    "# !pip install ./transformers/.\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/training_data/Model_BIO . #此落無法透過指令安裝 可從gs上面下載 gs://tbrain-tsmc/training_data/Model_BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Model_BIO were not used when initializing TFBertForTokenClassification: ['dropout_227']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForTokenClassification were not initialized from the model checkpoint at Model_BIO and are newly initialized: ['dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('Model_BIO', num_labels=3)\n",
    "model = TFBertForTokenClassification.from_pretrained('Model_BIO', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處長喝花酒不算貪污？ 二審逆轉重判10年2月前茂管處長許正雄涉索取回扣案，被高雄高分院改判有罪。（記者鮑建信攝）〔記者鮑建信／高雄報導〕交通部觀光局茂林國家風景區管理處前處長許正雄被控收回扣案，地院調查喝花酒沒有對價關係，又查無貪污證據，並指廠商為脫罪而認罪協商，判決無罪，案經高雄高分院審理後，下午大逆轉改判有罪，許處有期徒刑10年2月、副處長楊國聰處則維持無罪判決。檢方指出，2011年間起，當時茂林風管處長的許正雄向承攬「六龜區及賽嘉樂園遊憩服務設施工程」委託設計監造劉姓建築師，索討決標金369萬的5％回扣，許前後共涉嫌索賄81萬元。此外，吳姓、林姓包商以1587萬元承攬「六龜遊客中心舊址遊憩服務設施整建工程」後，涉嫌設宴招待許和副處長楊國聰後，又轉往「香格里拉」酒店喝花酒，案經檢方偵結，劉、吳、林3人緩起訴處分，許、楊被依貪瀆罪起訴。高雄地院調查，劉、吳、林等人的認罪證詞，積極事證有待商榷；至於招待官員喝花酒的請客文化存在已久，雖有悖公務員守份自持官箴，但業者為期建立「特別私交」招待的飲宴行為，非等同貪污治罪條例所處罰的職務上行為，如果無法證明官商間有對價關係，充其量只是違反公務員行政倫理範疇，認定罪證不足。不過，二審合議庭則採信劉證詞，認定劉和許共見面5次，每次見面後，劉就去領錢，共行賄許25萬元，因此改判有罪，可上訴。☆飲酒過量  有害健康  禁止酒駕☆\n",
      "['許正雄']\n",
      "正 雄 正 雄 正 雄\n",
      "國\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def extract_name(text, label):\n",
    "    l_label = np.argmax(label, -1)[0]\n",
    "    \n",
    "    answers = set()\n",
    "    answer = ''\n",
    "    \n",
    "    for i in np.where(l_label>0)[0].tolist():\n",
    "        name = tokenizer.decode([text.numpy()[i]])\n",
    "\n",
    "        if i in np.where(l_label==2)[0].tolist():\n",
    "            if answer != \"\":\n",
    "                answers.add(answer)\n",
    "                \n",
    "            answer = name\n",
    "        else:\n",
    "            answer += name\n",
    "\n",
    "    return answers\n",
    "\n",
    "###  這邊改成輸入的字串 test_string = ?\n",
    "%run DataProcessor.ipynb\n",
    "dp = DataProcessor()\n",
    "tid =  165 #72 139 150(空)\n",
    "titles, names, contexts = dp.get_all()\n",
    "test_string = titles[tid] + contexts[tid]\n",
    "print(test_string)\n",
    "print(names[tid])\n",
    "###\n",
    "\n",
    "MAX_LEN = 256\n",
    "part = len(test_string) // MAX_LEN\n",
    "i = 0\n",
    "answers = set()\n",
    "\n",
    "while i <= part:\n",
    "    if i == part:\n",
    "        inputs = tokenizer(test_string[i*MAX_LEN: ], return_tensors=\"tf\")\n",
    "    else:\n",
    "        inputs = tokenizer(test_string[i*MAX_LEN: (i+1)*MAX_LEN], return_tensors=\"tf\")\n",
    "        \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss, scores = outputs[:2]\n",
    "    \n",
    "    print(tokenizer.decode(input_ids.numpy()[np.argmax(scores, -1)>0]))\n",
    "    answers.update(extract_name(input_ids[0], scores))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "### 最終結果\n",
    "print(list(answers))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
