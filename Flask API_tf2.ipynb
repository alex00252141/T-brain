{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重要!! 請安裝以下資料\n",
    "# !git clone https://github.com/huggingface/transformers\n",
    "# !cd transformers\n",
    "# !pip install ./transformers/.\n",
    "# !gsutil -m cp -R gs://tbrain-tsmc/Model_BIO . #此落無法透過指令安裝 可從gs上面下載 gs://tbrain-tsmc/training_data/Model_BIO\n",
    "\n",
    "# config.json: https://storage.googleapis.com/tbrain-tsmc/Model_BIO/config.json\n",
    "# https://storage.googleapis.com/tbrain-tsmc/Model_BIO/tf_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras_bert\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "import datetime\n",
    "import hashlib\n",
    "from transformers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def extract_name(text, label):\n",
    "    l_label = np.argmax(label, -1)[0]\n",
    "    answers = set()\n",
    "    answer = ''\n",
    "    \n",
    "    for i in np.where(l_label>0)[0].tolist():\n",
    "        name = tokenizer.decode([text.numpy()[i]])\n",
    "\n",
    "        if i in np.where(l_label==2)[0].tolist():\n",
    "            if answer != \"\" and len(answer) > 1:\n",
    "                answers.add(answer)\n",
    "                \n",
    "            answer = name\n",
    "        else:\n",
    "            answer += name\n",
    "\n",
    "    return answers\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('Model_BIO', num_labels=3)\n",
    "model = TFBertForTokenClassification.from_pretrained('Model_BIO', config=config)\n",
    "\n",
    "app = Flask(__name__)\n",
    "####### PUT YOUR INFORMATION HERE #######\n",
    "CAPTAIN_EMAIL = 'sammo147@gmail.com'    #\n",
    "SALT = 'my_salt'                        #\n",
    "#########################################\n",
    "\n",
    "def generate_server_uuid(input_string):\n",
    "    \"\"\" Create your own server_uuid\n",
    "    @param input_string (str): information to be encoded as server_uuid\n",
    "    @returns server_uuid (str): your unique server_uuid\n",
    "    \"\"\"\n",
    "    s = hashlib.sha256()\n",
    "    data = (input_string+SALT).encode(\"utf-8\")\n",
    "    s.update(data)\n",
    "    server_uuid = s.hexdigest()\n",
    "    return server_uuid\n",
    "\n",
    "def predict(article):\n",
    "    \"\"\" Predict your model result\n",
    "    @param article (str): a news article\n",
    "    @returns prediction (list): a list of name\n",
    "    \"\"\"\n",
    "    MAX_LEN = 256\n",
    "    part = len(article) // MAX_LEN\n",
    "    i = 0\n",
    "    answers = set()\n",
    "\n",
    "    while i <= part:\n",
    "        if i == part:\n",
    "            inputs = tokenizer(article[i*MAX_LEN: ], return_tensors=\"tf\")\n",
    "        else:\n",
    "            inputs = tokenizer(article[i*MAX_LEN: (i+1)*MAX_LEN], return_tensors=\"tf\")\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "        outputs = model(inputs)\n",
    "        loss, scores = outputs[:2]\n",
    "        answers.update(extract_name(input_ids[0], scores))\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    prediction = _check_datatype_to_list(list(answers))\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def _check_datatype_to_list(prediction):\n",
    "    \"\"\" Check if your prediction is in list type or not. \n",
    "        And then convert your prediction to list type or raise error.\n",
    "        \n",
    "    @param prediction (list / numpy array / pandas DataFrame): your prediction\n",
    "    @returns prediction (list): your prediction in list type\n",
    "    \"\"\"\n",
    "    if isinstance(prediction, np.ndarray):\n",
    "        _check_datatype_to_list(prediction.tolist())\n",
    "    elif isinstance(prediction, pd.core.frame.DataFrame):\n",
    "        _check_datatype_to_list(prediction.values)\n",
    "    elif isinstance(prediction, list):\n",
    "        return prediction\n",
    "    raise ValueError('Prediction is not in list type.')\n",
    "\n",
    "@app.route('/healthcheck', methods=['POST'])\n",
    "def healthcheck():\n",
    "    \"\"\" API for health check \"\"\"\n",
    "    data = request.get_json(force=True)  \n",
    "    t = datetime.datetime.now()  \n",
    "    ts = str(int(t.utcnow().timestamp()))\n",
    "    server_uuid = generate_server_uuid(CAPTAIN_EMAIL+ts)\n",
    "    server_timestamp = t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return jsonify({'esun_uuid': data['esun_uuid'], 'server_uuid': server_uuid, 'captain_email': CAPTAIN_EMAIL, 'server_timestamp': server_timestamp})\n",
    "\n",
    "@app.route('/inference', methods=['POST'])\n",
    "def inference():\n",
    "    \"\"\" API that return your model predictions when E.SUN calls this API \"\"\"\n",
    "    data = request.get_json(force=True)  \n",
    "    esun_timestamp = data['esun_timestamp'] #自行取用\n",
    "    \n",
    "    t = datetime.datetime.now()  \n",
    "    ts = str(int(t.utcnow().timestamp()))\n",
    "    server_uuid = generate_server_uuid(CAPTAIN_EMAIL+ts)\n",
    "    \n",
    "    try:\n",
    "        answer = predict(data['news'])\n",
    "    except:\n",
    "        raise ValueError('Model error.')        \n",
    "    server_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return jsonify({'esun_timestamp': data['esun_timestamp'], 'server_uuid': server_uuid, 'answer': answer, 'server_timestamp': server_timestamp, 'esun_uuid': data['esun_uuid']})\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    app.run(host='0.0.0.0', port=8030)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
