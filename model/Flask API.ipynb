{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f5098cd0828>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f5098cd0828>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f5098c770f0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f5098c770f0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5098c77470>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f5098c77470>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5098c77668>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f5098c77668>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f50881b5dd8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f50881b5dd8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5098c77d30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f5098c77d30>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8030/ (Press CTRL+C to quit)\n",
      "[2020-07-13 09:49:48,563] ERROR in app: Exception on /inference [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-2-df16112c306a>\", line 110, in inference\n",
      "    answer = predict(data['news'])\n",
      "KeyError: 'news'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/flask/_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/flask/app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-2-df16112c306a>\", line 112, in inference\n",
      "    raise ValueError('Model error.')\n",
      "ValueError: Model error.\n",
      "104.199.204.133 - - [13/Jul/2020 09:49:48] \"\u001b[35m\u001b[1mPOST /inference HTTP/1.1\u001b[0m\" 500 -\n",
      "104.199.204.133 - - [13/Jul/2020 09:49:58] \"\u001b[37mPOST /healthcheck HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f50804a0f28> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f50804a0f28> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "104.199.204.133 - - [13/Jul/2020 09:50:08] \"\u001b[37mPOST /inference HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import keras_bert\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "import datetime\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# 請從google storage下載news_cheat_model_ver_01.h5\n",
    "model = keras.models.load_model( \"news_cheat_model_ver_01.h5\",custom_objects = keras_bert.get_custom_objects())\n",
    "\n",
    "folderPath      = \"\"\n",
    "dict_path       = os.path.join( folderPath , \"vocab.txt\"  )\n",
    "token_dict = keras_bert.load_vocabulary(dict_path)\n",
    "tokenizer  = keras_bert.Tokenizer(token_dict)\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    x = np.exp(x)\n",
    "    return x / np.sum(x)\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "####### PUT YOUR INFORMATION HERE #######\n",
    "CAPTAIN_EMAIL = 'sammo147@gmail.com'    #\n",
    "SALT = 'my_salt'                        #\n",
    "#########################################\n",
    "\n",
    "def generate_server_uuid(input_string):\n",
    "    \"\"\" Create your own server_uuid\n",
    "    @param input_string (str): information to be encoded as server_uuid\n",
    "    @returns server_uuid (str): your unique server_uuid\n",
    "    \"\"\"\n",
    "    s = hashlib.sha256()\n",
    "    data = (input_string+SALT).encode(\"utf-8\")\n",
    "    s.update(data)\n",
    "    server_uuid = s.hexdigest()\n",
    "    return server_uuid\n",
    "\n",
    "def predict(article):\n",
    "    \"\"\" Predict your model result\n",
    "    @param article (str): a news article\n",
    "    @returns prediction (list): a list of name\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(article)\n",
    "    x1, x2 = tokenizer.encode(first=article , max_len=400)\n",
    "    x1 = np.array([x1])\n",
    "    x2 = np.array([x2])\n",
    "    ps1, ps2 = model.predict( [x1,x2] )\n",
    "    ps1, ps2 = softmax(ps1[0]), softmax(ps2[0])\n",
    "    result = []\n",
    "    for i in range(10):\n",
    "        if ps1[np.flip( np.argsort(ps1) )[i]] > 0.2: \n",
    "\n",
    "            start = np.flip( np.argsort(ps1) )[i]\n",
    "            #start = np.flip( np.argsort(ps1) )[1]\n",
    "            end   = ps2[start:len(ps2)+1].argmax() + start\n",
    "            tokens[start:end+1]\n",
    "\n",
    "            result.append(''.join(tokens[start:end+1]))\n",
    "            result\n",
    "    prediction = _check_datatype_to_list(result)\n",
    "    return prediction\n",
    "\n",
    "def _check_datatype_to_list(prediction):\n",
    "    \"\"\" Check if your prediction is in list type or not. \n",
    "        And then convert your prediction to list type or raise error.\n",
    "        \n",
    "    @param prediction (list / numpy array / pandas DataFrame): your prediction\n",
    "    @returns prediction (list): your prediction in list type\n",
    "    \"\"\"\n",
    "    if isinstance(prediction, np.ndarray):\n",
    "        _check_datatype_to_list(prediction.tolist())\n",
    "    elif isinstance(prediction, pd.core.frame.DataFrame):\n",
    "        _check_datatype_to_list(prediction.values)\n",
    "    elif isinstance(prediction, list):\n",
    "        return prediction\n",
    "    raise ValueError('Prediction is not in list type.')\n",
    "\n",
    "@app.route('/healthcheck', methods=['POST'])\n",
    "def healthcheck():\n",
    "    \"\"\" API for health check \"\"\"\n",
    "    data = request.get_json(force=True)  \n",
    "    t = datetime.datetime.now()  \n",
    "    ts = str(int(t.utcnow().timestamp()))\n",
    "    server_uuid = generate_server_uuid(CAPTAIN_EMAIL+ts)\n",
    "    server_timestamp = t.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return jsonify({'esun_uuid': data['esun_uuid'], 'server_uuid': server_uuid, 'captain_email': CAPTAIN_EMAIL, 'server_timestamp': server_timestamp})\n",
    "\n",
    "@app.route('/inference', methods=['POST'])\n",
    "def inference():\n",
    "    \"\"\" API that return your model predictions when E.SUN calls this API \"\"\"\n",
    "    data = request.get_json(force=True)  \n",
    "    esun_timestamp = data['esun_timestamp'] #自行取用\n",
    "    \n",
    "    t = datetime.datetime.now()  \n",
    "    ts = str(int(t.utcnow().timestamp()))\n",
    "    server_uuid = generate_server_uuid(CAPTAIN_EMAIL+ts)\n",
    "    \n",
    "    try:\n",
    "        answer = predict(data['news'])\n",
    "    except:\n",
    "        raise ValueError('Model error.')        \n",
    "    server_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return jsonify({'esun_timestamp': data['esun_timestamp'], 'server_uuid': server_uuid, 'answer': answer, 'server_timestamp': server_timestamp, 'esun_uuid': data['esun_uuid']})\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    app.run(host='0.0.0.0', port=8030)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
